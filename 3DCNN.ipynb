{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Mallikarjunreddy3015/3DCNN/blob/main/3DCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-4M7_2i0XCWc"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import datetime as dt\n",
    "import h5py\n",
    "import numpy as np\n",
    "from skimage.segmentation import watershed\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2208\\2711954765.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Num GPUs Available: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJKp0F_ZcCje"
   },
   "outputs": [],
   "source": [
    "#  Copyright (C) 2012 Daniel Maturana\n",
    "#  This file is part of binvox-rw-py.\n",
    "#\n",
    "#  binvox-rw-py is free software: you can redistribute it and/or modify\n",
    "#  it under the terms of the GNU General Public License as published by\n",
    "#  the Free Software Foundation, either version 3 of the License, or\n",
    "#  (at your option) any later version.\n",
    "#\n",
    "#  binvox-rw-py is distributed in the hope that it will be useful,\n",
    "#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#  GNU General Public License for more details.\n",
    "#\n",
    "#  You should have received a copy of the GNU General Public License\n",
    "#  along with binvox-rw-py. If not, see <http://www.gnu.org/licenses/>.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "Binvox to Numpy and back.\n",
    ">>> import numpy as np\n",
    ">>> with open('chair.binvox', 'rb') as f:\n",
    "...     m1 = read_as_3d_array(f)\n",
    "...\n",
    ">>> m1.dims\n",
    "[32, 32, 32]\n",
    ">>> m1.scale\n",
    "41.133000000000003\n",
    ">>> m1.translate\n",
    "[0.0, 0.0, 0.0]\n",
    ">>> with open('chair_out.binvox', 'wb') as f:\n",
    "...     m1.write(f)\n",
    "...\n",
    ">>> with open('chair_out.binvox', 'rb') as f:\n",
    "...     m2 = read_as_3d_array(f)\n",
    "...\n",
    ">>> m1.dims==m2.dims\n",
    "True\n",
    ">>> m1.scale==m2.scale\n",
    "True\n",
    ">>> m1.translate==m2.translate\n",
    "True\n",
    ">>> np.all(m1.data==m2.data)\n",
    "True\n",
    ">>> with open('chair.binvox', 'rb') as f:\n",
    "...     md = read_as_3d_array(f)\n",
    "...\n",
    ">>> with open('chair.binvox', 'rb') as f:\n",
    "...     ms = read_as_coord_array(f)\n",
    "...\n",
    ">>> data_ds = .dense_to_sparse(md.data)\n",
    ">>> data_sd = sparse_to_dense(ms.data, 32)\n",
    ">>> np.all(data_sd==md.data)\n",
    "True\n",
    ">>> # the ordering of elements returned by numpy.nonzero changes with axis\n",
    ">>> # ordering, so to compare for equality we first lexically sort the voxels.\n",
    ">>> np.all(ms.data[:, np.lexsort(ms.data)] == data_ds[:, np.lexsort(data_ds)])\n",
    "True\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Voxels(object):\n",
    "    \"\"\" Holds a binvox model.\n",
    "    data is either a three-dimensional numpy boolean array (dense representation)\n",
    "    or a two-dimensional numpy float array (coordinate representation).\n",
    "    dims, translate and scale are the model metadata.\n",
    "    dims are the voxel dimensions, e.g. [32, 32, 32] for a 32x32x32 model.\n",
    "    scale and translate relate the voxels to the original model coordinates.\n",
    "    To translate voxel coordinates i, j, k to original coordinates x, y, z:\n",
    "    x_n = (i+.5)/dims[0]\n",
    "    y_n = (j+.5)/dims[1]\n",
    "    z_n = (k+.5)/dims[2]\n",
    "    x = scale*x_n + translate[0]\n",
    "    y = scale*y_n + translate[1]\n",
    "    z = scale*z_n + translate[2]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, dims, translate, scale, axis_order):\n",
    "        self.data = data\n",
    "        self.dims = dims\n",
    "        self.translate = translate\n",
    "        self.scale = scale\n",
    "        assert (axis_order in ('xzy', 'xyz'))\n",
    "        self.axis_order = axis_order\n",
    "\n",
    "    def clone(self):\n",
    "        data = self.data.copy()\n",
    "        dims = self.dims[:]\n",
    "        translate = self.translate[:]\n",
    "        return Voxels(data, dims, translate, self.scale, self.axis_order)\n",
    "\n",
    "    def write(self, fp):\n",
    "        write(self, fp)\n",
    "\n",
    "\n",
    "def read_header(fp):\n",
    "    \"\"\" Read binvox header. Mostly meant for internal use.\n",
    "    \"\"\"\n",
    "    line = fp.readline().strip()\n",
    "    if not line.startswith(b'#binvox'):\n",
    "        raise IOError('Not a binvox file')\n",
    "    dims = list(map(int, fp.readline().strip().split(b' ')[1:]))\n",
    "    translate = list(map(float, fp.readline().strip().split(b' ')[1:]))\n",
    "    scale = list(map(float, fp.readline().strip().split(b' ')[1:]))[0]\n",
    "    line = fp.readline()\n",
    "    return dims, translate, scale\n",
    "\n",
    "\n",
    "def read_as_3d_array(fp, fix_coords=True):\n",
    "    \"\"\" Read binary binvox format as array.\n",
    "    Returns the model with accompanying metadata.\n",
    "    Voxels are stored in a three-dimensional numpy array, which is simple and\n",
    "    direct, but may use a lot of memory for large models. (Storage requirements\n",
    "    are 8*(d^3) bytes, where d is the dimensions of the binvox model. Numpy\n",
    "    boolean arrays use a byte per element).\n",
    "    Doesn't do any checks on input except for the '#binvox' line.\n",
    "    \"\"\"\n",
    "    dims, translate, scale = read_header(fp)\n",
    "    raw_data = np.frombuffer(fp.read(), dtype=np.uint8)\n",
    "    # if just using reshape() on the raw data:\n",
    "    # indexing the array as array[i,j,k], the indices map into the\n",
    "    # coords as:\n",
    "    # i -> x\n",
    "    # j -> z\n",
    "    # k -> y\n",
    "    # if fix_coords is true, then data is rearranged so that\n",
    "    # mapping is\n",
    "    # i -> x\n",
    "    # j -> y\n",
    "    # k -> z\n",
    "    values, counts = raw_data[::2], raw_data[1::2]\n",
    "    data = np.repeat(values, counts).astype(bool)\n",
    "    data = data.reshape(dims)\n",
    "    if fix_coords:\n",
    "        # xzy to xyz TODO the right thing\n",
    "        data = np.transpose(data, (0, 2, 1))\n",
    "        axis_order = 'xyz'\n",
    "    else:\n",
    "        axis_order = 'xzy'\n",
    "    return Voxels(data, dims, translate, scale, axis_order)\n",
    "\n",
    "\n",
    "def read_as_coord_array(fp, fix_coords=True):\n",
    "    \"\"\" Read binary binvox format as coordinates.\n",
    "    Returns binvox model with voxels in a \"coordinate\" representation, i.e.  an\n",
    "    3 x N array where N is the number of nonzero voxels. Each column\n",
    "    corresponds to a nonzero voxel and the 3 rows are the (x, z, y) coordinates\n",
    "    of the voxel.  (The odd ordering is due to the way binvox format lays out\n",
    "    data).  Note that coordinates refer to the binvox voxels, without any\n",
    "    scaling or translation.\n",
    "    Use this to save memory if your model is very sparse (mostly empty).\n",
    "    Doesn't do any checks on input except for the '#binvox' line.\n",
    "    \"\"\"\n",
    "    dims, translate, scale = read_header(fp)\n",
    "    raw_data = np.frombuffer(fp.read(), dtype=np.uint8)\n",
    "\n",
    "    values, counts = raw_data[::2], raw_data[1::2]\n",
    "\n",
    "    sz = np.prod(dims)\n",
    "    index, end_index = 0, 0\n",
    "    end_indices = np.cumsum(counts)\n",
    "    indices = np.concatenate(([0], end_indices[:-1])).astype(end_indices.dtype)\n",
    "\n",
    "    values = values.astype(bool)\n",
    "    indices = indices[values]\n",
    "    end_indices = end_indices[values]\n",
    "\n",
    "    nz_voxels = []\n",
    "    for index, end_index in zip(indices, end_indices):\n",
    "        nz_voxels.extend(range(index, end_index))\n",
    "    nz_voxels = np.array(nz_voxels)\n",
    "    # TODO are these dims correct?\n",
    "    # according to docs,\n",
    "    # index = x * wxh + z * width + y; // wxh = width * height = d * d\n",
    "\n",
    "    x = nz_voxels / (dims[0]*dims[1])\n",
    "    zwpy = nz_voxels % (dims[0]*dims[1])  # z*w + y\n",
    "    z = zwpy / dims[0]\n",
    "    y = zwpy % dims[0]\n",
    "    if fix_coords:\n",
    "        data = np.vstack((x, y, z))\n",
    "        axis_order = 'xyz'\n",
    "    else:\n",
    "        data = np.vstack((x, z, y))\n",
    "        axis_order = 'xzy'\n",
    "\n",
    "    # return Voxels(data, dims, translate, scale, axis_order)\n",
    "    return Voxels(np.ascontiguousarray(data), dims, translate, scale, axis_order)\n",
    "\n",
    "\n",
    "def dense_to_sparse(voxel_data, dtype=int):\n",
    "    \"\"\" From dense representation to sparse (coordinate) representation.\n",
    "    No coordinate reordering.\n",
    "    \"\"\"\n",
    "    if voxel_data.ndim != 3:\n",
    "        raise ValueError('voxel_data is wrong shape; should be 3D array.')\n",
    "    return np.asarray(np.nonzero(voxel_data), dtype)\n",
    "\n",
    "\n",
    "def sparse_to_dense(voxel_data, dims, dtype=bool):\n",
    "    if voxel_data.ndim != 2 or voxel_data.shape[0] != 3:\n",
    "        raise ValueError('voxel_data is wrong shape; should be 3xN array.')\n",
    "    if np.isscalar(dims):\n",
    "        dims = [dims]*3\n",
    "    dims = np.array(dims, dtype=int).reshape(-1, 1)\n",
    "\n",
    "    # truncate to integers\n",
    "    xyz = voxel_data.astype(int)\n",
    "    # discard voxels that fall outside dims\n",
    "    valid_ix = ~np.any((xyz < 0) | (xyz >= dims), 0)\n",
    "    xyz = xyz[:, valid_ix]\n",
    "    out = np.zeros(dims.flatten(), dtype=dtype)\n",
    "    out[tuple(xyz)] = True\n",
    "    return out\n",
    "\n",
    "\n",
    "# def get_linear_index(x, y, z, dims):\n",
    "    # \"\"\" Assuming xzy order. (y increasing fastest.\n",
    "    # TODO ensure this is right when dims are not all same\n",
    "    # \"\"\"\n",
    "    # return x*(dims[1]*dims[2]) + z*dims[1] + y\n",
    "\n",
    "\n",
    "def write(voxel_model, fp):\n",
    "    \"\"\" Write binary binvox format.\n",
    "    Note that when saving a model in sparse (coordinate) format, it is first\n",
    "    converted to dense format.\n",
    "    Doesn't check if the model is 'sane'.\n",
    "    \"\"\"\n",
    "    if voxel_model.data.ndim == 2:\n",
    "        # TODO avoid conversion to dense\n",
    "        dense_voxel_data = sparse_to_dense(voxel_model.data, voxel_model.dims)\n",
    "    else:\n",
    "        dense_voxel_data = voxel_model.data\n",
    "\n",
    "    fp.write('#binvox 1\\n')\n",
    "    fp.write('dim '+' '.join(map(str, voxel_model.dims))+'\\n')\n",
    "    fp.write('translate '+' '.join(map(str, voxel_model.translate))+'\\n')\n",
    "    fp.write('scale '+str(voxel_model.scale)+'\\n')\n",
    "    fp.write('data\\n')\n",
    "    if not voxel_model.axis_order in ('xzy', 'xyz'):\n",
    "        raise ValueError('Unsupported voxel model axis order')\n",
    "\n",
    "    if voxel_model.axis_order == 'xzy':\n",
    "        voxels_flat = dense_voxel_data.flatten()\n",
    "    else:\n",
    "        voxels_flat = np.transpose(dense_voxel_data, (0, 2, 1)).flatten()\n",
    "\n",
    "    # keep a sort of state machine for writing run length encoding\n",
    "    state = voxels_flat[0]\n",
    "    ctr = 0\n",
    "    for c in voxels_flat:\n",
    "        if c == state:\n",
    "            ctr += 1\n",
    "            # if ctr hits max, dump\n",
    "            if ctr == 255:\n",
    "                fp.write(chr(state))\n",
    "                fp.write(chr(ctr))\n",
    "                ctr = 0\n",
    "        else:\n",
    "            # if switch state, dump\n",
    "            fp.write(chr(state))\n",
    "            fp.write(chr(ctr))\n",
    "            state = c\n",
    "            ctr = 1\n",
    "    # flush out remainders\n",
    "    if ctr > 0:\n",
    "        fp.write(chr(state))\n",
    "        fp.write(chr(ctr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgOqw1jAca12"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "\n",
    "def read_h5(split):\n",
    "    hf = h5py.File(split + \".h5\", 'r')\n",
    "    labels = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0,\n",
    "              16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0}\n",
    "\n",
    "    for key in list(hf.keys()):\n",
    "        with h5py.File('filename.h5', 'r') as f:\n",
    "    # # Load the 'x' and 'y' datasets from the file\n",
    "         x = np.array(f['group/x'], dtype=np.float32)\n",
    "         y = np.array(f['group/y'], dtype=np.int8)\n",
    "\n",
    "        print(f\"Group: {f}\")\n",
    "        print(f\"X: {np.shape(x)}\")\n",
    "        print(f\"Y: {y}\")\n",
    "\n",
    "        for label in y:\n",
    "            labels[label] += 1\n",
    "\n",
    "    hf.close()\n",
    "\n",
    "    print(labels)\n",
    "\n",
    "\n",
    "def split_dataset(split, samples):\n",
    "    random.shuffle(samples)\n",
    "    random.shuffle(samples)\n",
    "\n",
    "    train_idx = int(math.ceil(split[\"train\"] * len(samples)))\n",
    "    val_idx = int(math.ceil((split[\"val\"] * len(samples))) + train_idx)\n",
    "\n",
    "    train_list = samples[:train_idx]\n",
    "    val_list = samples[train_idx:val_idx]\n",
    "    test_list = samples[val_idx:]\n",
    "\n",
    "    return train_list, val_list, test_list\n",
    "\n",
    "\n",
    "def read_voxel_from_binvox(filepath, normalize=True):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        model = read_as_3d_array(f)\n",
    "    voxel = model.data\n",
    "\n",
    "    if normalize:\n",
    "        voxel = zero_centering_norm(voxel)\n",
    "\n",
    "    filename = filepath.split(\"\\\\\")[-1]\n",
    "    label = filename.split(\"_\")[0]\n",
    "    if  label.isdigit():\n",
    "     voxel = np.array(voxel, dtype=np.float32)\n",
    "     label = np.array(label, dtype=np.int8)\n",
    "\n",
    "    return voxel, label\n",
    "\n",
    "\n",
    "def zero_centering_norm(voxels):\n",
    "    norm = (voxels - 0.5) * 2\n",
    "    return norm\n",
    "\n",
    "\n",
    "def display_voxel(voxels):\n",
    "    fig = go.Figure(data=go.Volume(\n",
    "        x=voxels[:, :, :, 0],\n",
    "        y=voxels[:, :, :, 1],\n",
    "        z=voxels[:, :, :, 2],\n",
    "        value=voxels[:, :, :, 3],\n",
    "        isomin=0,\n",
    "        isomax=1,\n",
    "        opacity=0.1,\n",
    "        surface_count=21,\n",
    "        ))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVLe4wE3W6hp"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeatureNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(FeatureNet, self).__init__()\n",
    "\n",
    "        self.conv_1 = tf.keras.layers.Conv3D(filters=32, kernel_size=7, strides=(2, 2, 2), padding=\"same\", use_bias=True,\n",
    "                                             name=\"conv_1\")\n",
    "        self.conv_2 = tf.keras.layers.Conv3D(filters=32, kernel_size=5, strides=(1, 1, 1), padding=\"same\", use_bias=True,\n",
    "                                              name=\"conv_2\")\n",
    "        self.conv_3 = tf.keras.layers.Conv3D(filters=64, kernel_size=4, strides=(1, 1, 1), padding=\"same\", use_bias=True,\n",
    "                                              name=\"conv_3\")\n",
    "        self.conv_4 = tf.keras.layers.Conv3D(filters=64, kernel_size=3, strides=(1, 1, 1), padding=\"same\", use_bias=True,\n",
    "                                             name=\"conv_4\")\n",
    "\n",
    "        self.bn_1 = tf.keras.layers.BatchNormalization(name=\"batch_norm_1\")\n",
    "\n",
    "        self.pooling_1 = tf.keras.layers.MaxPool3D(pool_size=(2, 2, 2), strides=(2, 2, 2), padding=\"same\",\n",
    "                                                   name=\"pooling_1\")\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc_1 = tf.keras.layers.Dense(units=128, use_bias=True, name=\"fc_1\")\n",
    "        self.fc_2 = tf.keras.layers.Dense(units=num_classes, use_bias=True, name=\"fc_2\")\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "\n",
    "        x = self.conv_1(input)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv_3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.conv_4(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        x = self.pooling_1(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrKbrq2_X_KQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def dataloader_h5(file_path):\n",
    "#     hf = h5py.File(file_path, 'r')\n",
    "\n",
    "#     for key in list(hf.keys()):\n",
    "#         group = hf.get(key)\n",
    "#         x = tf.convert_to_tensor(np.array(group.get(\"x\"), dtype=np.float32))\n",
    "#         x = tf.Variable(x, dtype=tf.float32, name=\"x\")\n",
    " \n",
    "#         y = np.array(group[\"y\"], dtype=np.int8)\n",
    "\n",
    "#         yield x, y\n",
    "\n",
    "#     hf.close()\n",
    "def dataloader_h5(file_path):\n",
    "    hf = h5py.File(file_path, 'r')\n",
    "\n",
    "    for key in hf.keys():\n",
    "        group = hf.get(key)\n",
    "        if group is not None and isinstance(group, h5py.Group):\n",
    "            x = tf.convert_to_tensor(np.array(group[\"x\"], dtype=np.float32))\n",
    "            x = tf.Variable(x, dtype=tf.float32, name=\"x\")\n",
    "            y = np.array(group[\"y\"], dtype=np.int8)\n",
    "            yield x, y\n",
    "\n",
    "    hf.close()\n",
    "\n",
    "\n",
    "def read_voxel_from_binvox(filepath, normalize=True):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        model = read_as_3d_array(f)\n",
    "    voxel = model.data\n",
    "\n",
    "    if normalize:\n",
    "        voxel = zero_centering_norm(voxel)\n",
    "\n",
    "    filename = filepath.split(\"/\")[-1]\n",
    "    voxel = tf.convert_to_tensor(np.array(voxel, dtype=np.float32))\n",
    "    voxel = tf.Variable(voxel, dtype=tf.float32, name=\"x\")\n",
    "\n",
    "\n",
    "    return voxel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5iDnJpca2av"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aL2vXy-PWyIu",
    "outputId": "ccabe1f9-0f42-4fd4-cc4d-946141810bd3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "    grads = [tf.clip_by_norm(g, 1.0) for g in grads]\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    train_loss_metric.update_state(loss_value)\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "\n",
    "\n",
    "def val_step(x, y):\n",
    "    val_logits = model(x, training=False)\n",
    "    loss_value = loss_fn(y, val_logits)\n",
    "\n",
    "    val_loss_metric.update_state(loss_value)\n",
    "    val_acc_metric.update_state(y, val_logits)\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    # User Parameters\n",
    "    num_classes = 24\n",
    "    num_epochs = 30\n",
    "    learning_rate = 0.001\n",
    "    training_set_path = \"train.h5\"\n",
    "    val_set_path = \"val.h5\"\n",
    "\n",
    "    decay_rate = learning_rate / num_epochs\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(learning_rate,\n",
    "                                                                 decay_steps=100000, decay_rate=decay_rate)\n",
    "\n",
    "    save_name = f'featurenet_date_{dt.datetime.now().strftime(\"%Y-%m-%d\")}'\n",
    "\n",
    "    model = FeatureNet(num_classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    summary_writer = tf.summary.create_file_writer(f'./log/{save_name}')\n",
    "\n",
    "    train_loss_metric = tf.keras.metrics.Mean()\n",
    "    train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    val_loss_metric = tf.keras.metrics.Mean()\n",
    "    val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "    min_val_loss = 0.0\n",
    "    min_train_loss = 0.0\n",
    "    max_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    max_epoch = 0\n",
    "\n",
    "    for epoch in tf.range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1} of {num_epochs}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_dataloader = dataloader_h5(training_set_path)\n",
    "        val_dataloader = dataloader_h5(val_set_path)\n",
    "\n",
    "        with summary_writer.as_default():\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataloader):\n",
    "                one_hot_y = tf.one_hot(y_batch_train, depth=num_classes)\n",
    "                train_step(x_batch_train, one_hot_y)\n",
    "\n",
    "                # Log every 20 batches.\n",
    "                if step % 100 == 0:\n",
    "                    print(\n",
    "                        \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                        % (step, float(train_loss_metric.result()))\n",
    "                    )\n",
    "\n",
    "            train_loss = train_loss_metric.result()\n",
    "            train_acc = train_acc_metric.result()\n",
    "\n",
    "            tf.summary.scalar('train_loss', train_loss, step=optimizer.iterations)\n",
    "            tf.summary.scalar('train_acc', train_acc, step=optimizer.iterations)\n",
    "\n",
    "            train_loss_metric.reset_states()\n",
    "            train_acc_metric.reset_states()\n",
    "\n",
    "            print(f\"Train loss={train_loss}, Train acc={train_acc}\")\n",
    "\n",
    "            for x_batch_val, y_batch_val in val_dataloader:\n",
    "                one_hot_y = tf.one_hot(y_batch_val, depth=num_classes)\n",
    "                val_step(x_batch_val, one_hot_y)\n",
    "\n",
    "            val_loss = val_loss_metric.result()\n",
    "            val_acc = val_acc_metric.result()\n",
    "\n",
    "            if val_acc > max_val_acc:\n",
    "                min_val_loss = float(val_loss)\n",
    "                min_train_loss = float(train_loss)\n",
    "                max_train_acc = float(train_acc)\n",
    "                max_val_acc = float(val_acc)\n",
    "                model.save_weights(f\"checkpoint/{save_name}.ckpt\")\n",
    "                max_epoch = epoch\n",
    "\n",
    "            tf.summary.scalar('val_loss', val_loss, step=optimizer.iterations)\n",
    "            tf.summary.scalar('val_acc', val_acc, step=optimizer.iterations)\n",
    "\n",
    "            val_loss_metric.reset_states()\n",
    "            val_acc_metric.reset_states()\n",
    "\n",
    "            print(f\"Val loss={val_loss}, Val acc={val_acc}\")\n",
    "            print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
    "\n",
    "    print(f\"Epoch={max_epoch+1}, Max train acc={max_train_acc}, Max val acc={max_val_acc}\")\n",
    "    print(f\"Train loss={min_train_loss}, Val loss={min_val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lx7jSathXL1C",
    "outputId": "5fadd464-1d83-4824-fca7-9b43b34f6878"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def test_step(x, y):\n",
    "    test_logits = model(x, training=False)\n",
    "    loss_value = loss_fn(y, test_logits)\n",
    "\n",
    "    y_true = np.argmax(y.numpy(), axis=1)\n",
    "    y_pred = np.argmax(test_logits.numpy(), axis=1)\n",
    "\n",
    "    test_loss_metric.update_state(loss_value)\n",
    "    test_acc_metric.update_state(y, test_logits)\n",
    "    test_precision_metric.update_state(y, test_logits)\n",
    "    test_recall_metric.update_state(y, test_logits)\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def test_step_no_labels(x):\n",
    "    test_logits = model(x, training=False)\n",
    "    y_pred = np.argmax(test_logits.numpy(), axis=1)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    # User Parameters\n",
    "    num_classes = 24\n",
    "    test_set_path = \"/content/drive/MyDrive/3DCNN/test.h5\"\n",
    "    checkpoint_path = \"/content/checkpoint/featurenet_date_2023-02-20.ckpt\"\n",
    "\n",
    "    model = FeatureNet(num_classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    test_loss_metric = tf.keras.metrics.Mean()\n",
    "    test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    test_precision_metric = tf.keras.metrics.Precision()\n",
    "    test_recall_metric = tf.keras.metrics.Recall()\n",
    "\n",
    "    model.load_weights(checkpoint_path)\n",
    "    test_dataloader = dataloader_h5(test_set_path)\n",
    "\n",
    "    y_true_total = []\n",
    "    y_pred_total = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for x_batch_test, y_batch_test in test_dataloader:\n",
    "        one_hot_y = tf.one_hot(y_batch_test, depth=num_classes)\n",
    "        y_true, y_pred = test_step(x_batch_test, one_hot_y)\n",
    "        y_true_total = np.append(y_true_total, y_true)\n",
    "        y_pred_total = np.append(y_pred_total, y_pred)\n",
    "\n",
    "    test_loss = test_loss_metric.result()\n",
    "    test_acc = test_acc_metric.result()\n",
    "    test_precision = test_precision_metric.result()\n",
    "    test_recall = test_recall_metric.result()\n",
    "\n",
    "    test_loss_metric.reset_states()\n",
    "    test_acc_metric.reset_states()\n",
    "    test_precision_metric.reset_states()\n",
    "    test_recall_metric.reset_states()\n",
    "\n",
    "    print(f\"Test loss={test_loss}, Test acc={test_acc}, Precision={test_precision}, Recall={test_recall}\")\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 786
    },
    "id": "BaNQDu5HXmKI",
    "outputId": "771890a8-64b0-474f-d0e5-4d70f643accc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_seg_samples(labels):\n",
    "\n",
    "    samples = np.zeros((0, labels.shape[0], labels.shape[1], labels.shape[2]))\n",
    "\n",
    "    for i in range(1, np.max(labels.astype(int)) + 1):\n",
    "        idx = np.where(labels == i)\n",
    "\n",
    "        if len(idx[0]) == 0:\n",
    "            continue\n",
    "\n",
    "        cursample = np.ones(labels.shape)\n",
    "        cursample[idx] = 0\n",
    "        cursample = np.expand_dims(cursample, axis=0)\n",
    "        samples = np.append(samples, cursample, axis=0)\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "def decomp_and_segment(sample):\n",
    "   \n",
    "    blobs = ~sample\n",
    "    final_labels = np.zeros(blobs.shape)\n",
    "    all_labels = measure.label(blobs)\n",
    "    display_features(all_labels)  # Display connected component\n",
    "    all_labels = np.array(all_labels)\n",
    "    for i in range(1, np.max(all_labels) + 1):\n",
    "        mk = (all_labels == i)\n",
    "        distance = ndi.distance_transform_edt(mk)\n",
    "        distance_arr = np.array(distance)\n",
    "        labels = watershed(-distance_arr)\n",
    "\n",
    "        max_val = np.max(final_labels) + 1\n",
    "        idx = np.where(mk)\n",
    "\n",
    "        final_labels[idx] += (labels[idx] + max_val)\n",
    "\n",
    "    # display_features(final_labels) # Display watershed\n",
    "    results = get_seg_samples(final_labels)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_bounding_box(voxel):\n",
    "    a = np.where(voxel != 0)\n",
    "    bbox = np.min(a[0]), np.max(a[0]), np.min(\n",
    "        a[1]), np.max(a[1]), np.min(a[2]), np.max(a[2])\n",
    "    return bbox\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def zero_centering_norm(voxels):\n",
    "    norm = (voxels - 0.5) * 2\n",
    "    return norm\n",
    "\n",
    "\n",
    "def load_voxel(file_path):\n",
    "    hf = h5py.File(file_path, 'r')\n",
    "    x = None\n",
    "\n",
    "    for key in list(hf.keys()):\n",
    "        with h5py.File('filename.h5', 'r') as f:\n",
    "    # # Load the 'x' and 'y' datasets from the file\n",
    "         x = np.array(f['group/x'], dtype=np.float32)\n",
    "         y = np.array(f['group/y'], dtype=np.int8)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def display_voxel(voxels, color):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(*np.indices(voxels.shape), c=color)\n",
    "    plt.grid(b=None)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def display_features(features):\n",
    "    color_code = {0: \"red\", 1: \"blue\", 2: \"green\", 3: \"orange\",\n",
    "                  4: \"grey\", 5: \"yellow\", 6: \"pink\", 7: \"purple\" ,8:\"white\"}\n",
    "    unique, counts = np.unique(features, return_counts=True)\n",
    "    colors = np.empty(features.shape, dtype=object)\n",
    "\n",
    "    for i in range(len(unique)):\n",
    "        colors[np.where(features == unique[i], True, False)] = color_code[i]\n",
    "\n",
    "    colors = colors.flatten()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(*np.indices(features.shape), c=colors)\n",
    "    plt.grid(b=None)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def display_machining_feature(index, count):\n",
    "    feature_code = {0: \"Chamfer\", 1: \"Through Hole\", 2: \"Triangular Passage\", 3: \"Rectangular Passage\", 4: \"6-Sided Passage\",\n",
    "                    5: \"Triangular Through Slot\", 6: \"Rectangular Through Slot\", 7: \"Circular Through Slot\",\n",
    "                    8: \"Rectangular Through Step\", 9: \"2-Sided Passage\", 10: \"Slanted Through Step\",\n",
    "                    11: \"O-Ring\", 12: \"Blind Hole\", 13: \"Triangular Pocket\",\n",
    "                    14: \"Rectangular Pocket\", 15: \"6-Sided Pocket\", 16: \"Circular End Pocket\",\n",
    "                    17: \"Rectangular Blind Slot\", 18: \"Vertical Circular End Blind Slot\", 19: \"Horizontal Circular End Blind Slot\",\n",
    "                    20: \"Triangular Blind Step\", 21: \"Circular Blind Step\", 22: \"Rectangular Blind Step\", 23: \"Round\"}\n",
    "\n",
    "    print(f\"[{count}] -> Index: {index}, Machining Feature: {feature_code[index[0]]}\")\n",
    "\n",
    "\n",
    "if 1==1:\n",
    "    # User Parameters\n",
    "    num_classes = 24\n",
    "    voxel_resolution = 64\n",
    "    binvox_path = \"/content/drive/MyDrive/3DCNN/0_22.binvox\"\n",
    "    checkpoint_path = \"/content/checkpoint/featurenet_date_2023-02-20.ckpt\"\n",
    "\n",
    "    with open(binvox_path, 'rb') as f:\n",
    "        model = read_as_3d_array(f)\n",
    "    voxel = model.data\n",
    "    #display_voxel(voxel, \"grey\") # Display voxel model\n",
    "\n",
    "    features = decomp_and_segment(voxel)\n",
    "\n",
    "    model = FeatureNet(num_classes=num_classes)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "    for i, feature in enumerate(features):\n",
    "        input = zero_centering_norm(feature)\n",
    "        x = tf.Variable(input, dtype=tf.float32)\n",
    "        x = tf.reshape(x, [1, 1, voxel_resolution,\n",
    "        \n",
    "                    voxel_resolution, voxel_resolution])\n",
    "        test_logits = model(x, training= False)\n",
    "        y_pred = np.argmax(test_logits.numpy(), axis=1)\n",
    "        display_machining_feature(y_pred, i)\n",
    "        # display_voxel(feature, \"grey\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPYzRvlNzcONR2UwdnsGBxD",
   "include_colab_link": true,
   "mount_file_id": "12E90XoUuGTU7mzAfFqBuddZH3-pZdbVh",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
